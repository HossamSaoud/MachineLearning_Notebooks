{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2acb891493a64e0386a40d61f7c0b8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ee07ba1aae44e68b3d3f824e3843827",
              "IPY_MODEL_9434cfe4b7464471872882270ca37712",
              "IPY_MODEL_07def3e4dc614226a68bb5c774aebc4c"
            ],
            "layout": "IPY_MODEL_8b8de6dfebe94ffb8f07fc54b4a2cdce"
          }
        },
        "6ee07ba1aae44e68b3d3f824e3843827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3169ad80c83544319c3ad1460680d66b",
            "placeholder": "​",
            "style": "IPY_MODEL_7a3f757fb323485084c1cf2d019f646c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9434cfe4b7464471872882270ca37712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e871be6f18444588b2b97b98275ecd1b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c390a3943a447789bdd4d5094347dd3",
            "value": 2
          }
        },
        "07def3e4dc614226a68bb5c774aebc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f47ddbaa7434f92a766d8bd88f2e744",
            "placeholder": "​",
            "style": "IPY_MODEL_5b20a58a81c941a1aa311a2a676c1abe",
            "value": " 2/2 [00:21&lt;00:00,  9.09s/it]"
          }
        },
        "8b8de6dfebe94ffb8f07fc54b4a2cdce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3169ad80c83544319c3ad1460680d66b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3f757fb323485084c1cf2d019f646c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e871be6f18444588b2b97b98275ecd1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c390a3943a447789bdd4d5094347dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f47ddbaa7434f92a766d8bd88f2e744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b20a58a81c941a1aa311a2a676c1abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HossamSaoud/MachineLearning_Notebooks/blob/main/Minutes_Meeting_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KXa_RAPjlVui"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "KPpgEo_Zm0Li"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "nIiVHDaqm2ym"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MODEL = \"whisper-1\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\""
      ],
      "metadata": {
        "id": "L-iNtpLnm8ud"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "pPNQtdrQnfVT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio_file = open(audio_filename, \"rb\")\n",
        "# transcript = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file)"
      ],
      "metadata": {
        "id": "LhcF46bnnyoH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(transcript.text)"
      ],
      "metadata": {
        "id": "Bcdv8xDeW2Gg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# system_message =\"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        " #user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcript}\"\n"
      ],
      "metadata": {
        "id": "KQ9LqDg9n47k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =[]"
      ],
      "metadata": {
        "id": "-tWUs567oRRk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function to load model and tokenizer\n",
        "def load_model(model_name):\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quant_config, trust_remote_code=True)\n",
        "    return tokenizer, model"
      ],
      "metadata": {
        "id": "-0PBGfUVocWe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_full_response (tokenizer,model,user_input, max_tokens1=1000):\n",
        "  global messages\n",
        "  messages.append({\"role\": \"user\", \"content\" : user_input})\n",
        "  #pass inputs to tokenzier then pass tokens to the model and generate response, then print it\n",
        "  inputs = tokenizer.apply_chat_template(messages,return_tensors=\"pt\",add_generation_prompt=True).to(\"cuda\")\n",
        "  outputs = model.generate(inputs,max_new_tokens=max_tokens)\n",
        "  response = tokenizer.decode(outputs[0])\n",
        "  messages.append({\"role\":\"assistant\", \"content\": response})\n",
        "  print(response)\n",
        "  del tokenizer, model,inputs,outputs\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hSUz00xtoo9e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_stream_low_level(tokenizer,model,user_input, max_tokens=1000):\n",
        "  global messages\n",
        "  messages.append({\"role\": \"user\", \"content\" : user_input})\n",
        "  messages.append({\"role\":\"assistant\", \"content\": \"\"})\n",
        "  input_ids = tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")\n",
        "  response = \"\"\n",
        "  for _ in range(max_tokens):\n",
        "    outputs = model(input_ids)\n",
        "    next_token_id = outputs.logits[: ,-1].argmax(dim=-1).unsqueeze(-1)\n",
        "    input_ids = torch.cat([input_ids,next_token_id[0]],dim=-1)\n",
        "    next_token = tokenizer.decode(next_token_id[0])\n",
        "    print(next_token,end=\"\",flush=True)\n",
        "    response+=next_token\n",
        "    if next_token_id.item() == tokenizer.eos_token_id:\n",
        "      break\n",
        "    messages.append({\"role\":\"assistant\", \"content\": response})\n",
        "    print()\n",
        "    del tokenizer, model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "t4JCmNF8rHpA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "def generate_stream_high_level(tokenizer,model,user_input, max_tokens=1000):\n",
        "  global messages\n",
        "  messages.append({\"role\": \"user\", \"content\" : user_input})\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")\n",
        "  streamer = TextIteratorStreamer(\n",
        "      tokenizer,\n",
        "      skip_prompt = True,\n",
        "      decode_kwargs = {\"skip_special_tokens\":True}\n",
        "  )\n",
        "\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,\n",
        "      kwargs={\n",
        "          \"inputs\":inputs,\n",
        "          \"max_new_tokens\":max_tokens,\n",
        "          \"streamer\":streamer\n",
        "      }\n",
        "  )\n",
        "  thread.start()\n",
        "\n",
        "  for text_chunk in streamer:\n",
        "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\",\"\")\n",
        "    print(filtered_chunk,end=\"\",flush=True)\n",
        "  print()\n",
        "  del tokenizer, model ,inputs,outputs\n",
        "  torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "id": "8y7oVZvasJ-y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model = load_model(GEMMA2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2acb891493a64e0386a40d61f7c0b8bc",
            "6ee07ba1aae44e68b3d3f824e3843827",
            "9434cfe4b7464471872882270ca37712",
            "07def3e4dc614226a68bb5c774aebc4c",
            "8b8de6dfebe94ffb8f07fc54b4a2cdce",
            "3169ad80c83544319c3ad1460680d66b",
            "7a3f757fb323485084c1cf2d019f646c",
            "e871be6f18444588b2b97b98275ecd1b",
            "9c390a3943a447789bdd4d5094347dd3",
            "4f47ddbaa7434f92a766d8bd88f2e744",
            "5b20a58a81c941a1aa311a2a676c1abe"
          ]
        },
        "id": "v27ALuxAtYTu",
        "outputId": "6d1a0ad0-fee3-4756-9583-b6830a39439f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2acb891493a64e0386a40d61f7c0b8bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate_full_response(tokenizer,model,user_prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "RWyAM0_ouaJA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del tokenizer, model\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "PbcVDMvWSEeh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer, model = load_model(GEMMA2)"
      ],
      "metadata": {
        "id": "jlJgUBurTVLw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate_stream_low_level(tokenizer,model,user_prompt)"
      ],
      "metadata": {
        "id": "J9F_ipIvuz4J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate_stream_high_level(tokenizer,model,user_prompt)"
      ],
      "metadata": {
        "id": "bd46TvZAu2od"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_stream(user_input):\n",
        "#   global tokenizer,model,messages,max_tokens\n",
        "#   messages.append({\"role\": \"user\", \"content\" : user_input})\n",
        "#   input_ids = tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")\n",
        "#   result=\"\"\n",
        "\n",
        "#   for _ in range(max_tokens):\n",
        "#     outputs = model(inputs_id)\n",
        "#     next_token_id = outputs.logits[: ,-1].argmax(dim=-1).unsqueeze(-1)\n",
        "#     input_ids = torch.cat([input_ids,next_token_id[0]],dim=-1)\n",
        "#     next_token = tokenizer.decode(next_token_id[0],skip_special_tokens=True)\n",
        "#     result+=next_token\n",
        "#     yield result\n",
        "#     if next_token_id.item() == tokenizer.eos_token_id:\n",
        "#       break\n",
        "\n",
        "#     messages.append({\"role\":\"assistant\", \"content\": \"result\"})"
      ],
      "metadata": {
        "id": "v-nDSFlsu3-o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 1000"
      ],
      "metadata": {
        "id": "fpbwpJo3XZWu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio_filename):\n",
        "  audio_file = open(audio_filename, \"rb\")\n",
        "  history = []\n",
        "  transcript = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file)\n",
        "  user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcript.text}\"\n",
        "  yield from generate_stream_optimized(user_prompt,history)\n",
        "\n"
      ],
      "metadata": {
        "id": "KIsYLKaeffnk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxYNFc5ijP0J"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimize the streaming function for gradio (using TextIteratorStreamer)\n",
        "def generate_stream_optimized(user_input,history):\n",
        "  # Global variables for modifications\n",
        "  global tokenizer, model, max_tokens\n",
        "\n",
        "  # Step 1: Append the user's new message to the conversation history\n",
        "  messages = history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "  # Step 2: Prepare the inputs for the model by applying the chat template\n",
        "  # The inputs include the conversation history and the user's latest message\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  # we skip using TextStreamer() here cause it streams back results to stdout and thats not what we want in gradio app\n",
        "  # we use TextIteratorStreamer() instead\n",
        "\n",
        "  # Step 3: Initialize the TextIteratorStreamer\n",
        "  streamer = TextIteratorStreamer(\n",
        "      tokenizer,\n",
        "      skip_prompt=True,  # Ensures that the input prompt is not repeatedly included in the streamed output.\n",
        "      decode_kwargs={\"skip_special_tokens\": True}  # Filters out special tokens (e.g., <s>, </s>, <pad>, <cls>, <sep>) from the generated text.\n",
        "  )\n",
        "\n",
        "  # Step 4: Create a thread to run the generation process in the background\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,  # Specifies that the model's `generate` method will be run in the thread.\n",
        "      kwargs={                           # Passes the arguments required for text generation\n",
        "          \"inputs\": inputs,              # The tokenized input prompt for the model.\n",
        "          \"max_new_tokens\": max_tokens,  # Limits the number of tokens to be generated.\n",
        "          \"streamer\": streamer           # The TextIteratorStreamer to handle streaming the output.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  # Step 5: Start the thread to begin the generation process\n",
        "  thread.start()\n",
        "\n",
        "  # Step 6: Initialize an empty string to accumulate the growing output\n",
        "  accumulated_reply = \"\"\n",
        "\n",
        "  # Step 7: Stream the output progressively\n",
        "  for text_chunk in streamer:  # Iterate over each chunk of text streamed by the model\n",
        "      # Filter out any unexpected special tokens manually if they appear to ensure a clean output\n",
        "      # `<|eot_id|>` is a special token (e.g., end-of-text marker) that may still appear in some outputs\n",
        "      filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")\n",
        "\n",
        "      # Append the filtered chunk to the accumulated text that holds all the generated text seen so far\n",
        "      accumulated_reply += filtered_chunk\n",
        "\n",
        "      # Yield the accumulated text to the calling function/UI for progressive updates,\n",
        "      # ensuring the output is continuously refreshed with new content\n",
        "      yield accumulated_reply\n",
        "\n",
        "  # Step 8: Append the final assistant response to the conversation history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": accumulated_reply})"
      ],
      "metadata": {
        "id": "HolnqBwzwGxr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wC3IJryMVLEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c127005e-6611-41d6-d206-e32dad1e0748"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.12.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.5.4 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.5.4)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "# # Gradio interface\n",
        "# interface = gr.Interface(\n",
        "#     fn=process_audio,  # Function to process the uploaded audio\n",
        "#     inputs=gr.Audio(type=\"filepath\", label=\"Upload Audio\"),  # Audio input\n",
        "#     outputs = [gr.Textbox(gr.Markdown(label = \"result\"))],  # Markdown supports dynamic content updates\n",
        "#     title=\"Audio File Uploader\",\n",
        "#     description=\"Upload an audio file and press Submit to get a streamed response.\",\n",
        "#     flagging_mode= \"never\"\n",
        "# )\n",
        "\n",
        "# # Launch the interface\n",
        "# interface.launch(debug=True)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with AI (Streaming Enabled)\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        user_input = gr.Audio(type=\"filepath\", label=\"Upload Audio\")\n",
        "        output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
        "        send_button = gr.Button(\"Send\")\n",
        "\n",
        "    send_button.click(fn=process_audio, inputs=user_input, outputs=output_box)\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "x5WMdq7vXBJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "outputId": "fae7c744-bf13-43ad-8f86-ae1ff9e4fe54"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://47feff6fde8f7c884b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://47feff6fde8f7c884b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://47feff6fde8f7c884b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gStGVcLT158D"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}